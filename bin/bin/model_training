#!/usr/bin/env python3.8
import sys
import os
import argparse
from Bio import SeqIO
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV
import datetime
import lightgbm as lgb
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from joblib import dump
from bayes_opt import BayesianOptimization


G = ['A', 'T', 'C', 'G']
def parse_args(args):
    parser = argparse.ArgumentParser(description="model training")
    parser._action_groups.pop()
    required_args = parser.add_argument_group('required arguments')
    required_args.add_argument('-t', required = True, help='Data used for training model')
    required_args.add_argument('-o', required = True, help='Output directory')
    required_args.add_argument('-k', required=True)  
    if len(sys.argv)==1:
        parser.print_help(sys.stderr)
        sys.exit(1)

    return parser.parse_args()

def generate_feature_vector(sequence_file, k, label):
    kmers = []
    feature_vector = {}
    feature_matrix = []
    label_vector = []

    with open(sequence_file, 'r') as f:
        for record in SeqIO.parse(f, "fasta"):
            seq = str(record.seq)
            kmer = []
            kmers = [seq[i:i + k] for i in range(len(seq) - k + 1)]

            for i1 in G:
                for i2 in G:
                    for i3 in G:
                        for i4 in G:
                            for i5 in G:
                                km = i1 + i2 + i3 + i4 + i5
                                feature_vector[km] = 0

            for kmer in kmers:
                if kmer not in feature_vector:
                    continue
                else:
                    feature_vector[kmer] += 1

            feature_matrix.append(list(feature_vector.values()))
            label_vector.append(label)

    return feature_matrix, label_vector

def generate_feature_matrix(sequence_folder, k, label):
    files = os.listdir(sequence_folder)
    feature_matrix = []
    label_vector = []

    for file in files:
        fm, lv = generate_feature_vector(os.path.join(sequence_folder, file), k, label)
        feature_matrix.extend(fm)
        label_vector.extend(lv)

    return feature_matrix, label_vector

def generate_feature_matrix_multiple(sequence_folders, k):
    feature_matrix = []
    label_vector = []
    weight = dict()
    files = os.listdir(sequence_folders)

    for sequence_folder in files:
        fm, lv = generate_feature_matrix(os.path.join(sequence_folders, sequence_folder), k, sequence_folder)
        feature_matrix.extend(fm)
        label_vector.extend(lv)
        label = lv[0]
        weight[label] = compute_sample_weight(class_weight='balanced', y=lv).astype(int)


    return np.array(feature_matrix), np.array(label_vector), weight

def main():

    args = parse_args(sys.argv[1:])
    sequence_folders=args.t
    k = int (args.k)
    model_dir=args.o


    time1 = datetime.datetime.now()

    feature_matrix, label_vector, weight = generate_feature_matrix_multiple(sequence_folders, k)


    scaler = StandardScaler()
    feature_matrix_scaled = scaler.fit_transform(feature_matrix)

    pca = PCA(n_components=0.95) 
    feature_matrix_pca = pca.fit_transform(feature_matrix_scaled)


    X_train, X_test, y_train1, y_test1 = train_test_split(feature_matrix_pca, label_vector, test_size=0.2,
                                                        random_state=42)
    weight_train = compute_sample_weight(class_weight='balanced', y=y_train1)



    res = np.load('label.npy',allow_pickle=True)
    label=res.item()
    y_train=[]
    y_test=[]
    for vector_train in y_train1:
        y_train .append( label[vector_train])
        y_train = [int(yt) for yt in y_train]

    for vector_test in y_test1:
        y_test .append( label[vector_test])

    param_grid = {
        'max_depth': [6, 7],
        'min_child_weight': [1, 5],
        'subsample': [0.5],
        'colsample_bytree': [0.6, 0.8],
        'num_class': [210],
        'learning_rate': [0.1, 0.05, 0.01],
        'n_estimators': [221],
        'verbose': [-1],
        'num_leaves': [31, 60],
        'focal_loss': ['multiclass'],
    }


    model = lgb.LGBMClassifier(objective='multiclass')
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)
    grid_search.fit(X_train, y_train, sample_weight=weight_train)


    best_model = grid_search.best_estimator_
    y_pred_lgbm = best_model.predict(X_train)


    lr = LogisticRegression()
    svc = SVC()
    rf = RandomForestClassifier()

    lr.fit(X_train, y_train)
    svc.fit(X_train, y_train)
    rf.fit(X_train, y_train)

    y_pred_lr = lr.predict(X_train)
    y_pred_svc = svc.predict(X_train)
    y_pred_rf = rf.predict(X_train)

    label_encoder = LabelEncoder()

    X_train_combined = np.concatenate((X_train, y_pred_lgbm.reshape(-1, 1),
                                    y_pred_lr.reshape(-1, 1),
                                    y_pred_svc.reshape(-1, 1),
                                    y_pred_rf.reshape(-1, 1)), axis=1)


    fusion_model = lgb.LGBMClassifier(objective='multiclass', num_class=210)

    param_grid_fusion = {
        'max_depth': [6, 7],
        'min_child_weight': [1, 5],
        'subsample': [0.5],
        'colsample_bytree': [0.6, 0.8],
        'num_class': [210],
        'learning_rate': [0.1, 0.05, 0.01],
        'n_estimators': [221],
        'verbose': [-1],
        'num_leaves': [31, 60],
        'focal_loss': ['multiclass'],
    }


    dtrain = lgb.Dataset(X_train_combined, label=y_train, weight=weight_train)
    def lgb_eval(max_depth, min_child_weight, subsample, colsample_bytree, learning_rate, n_estimators, num_leaves):
        params = {
            'max_depth': int(max_depth),
            'min_child_weight': int(min_child_weight),
            'subsample': subsample,
            'colsample_bytree': colsample_bytree,
            'learning_rate': learning_rate,
            'n_estimators': int(n_estimators),
            'num_leaves': int(num_leaves),
            'objective': 'multiclass',
            'num_class': 210,
            'verbose': -1
        }

        cv_result = lgb.cv(params, dtrain, num_boost_round=100, nfold=5, stratified=True, verbose_eval=None, early_stopping_rounds=10)
        return -cv_result['multi_logloss-mean'][-1]

    param_space = {
        'max_depth': (6, 7),
        'min_child_weight': (1, 5),
        'subsample': (0.5, 1.0),
        'colsample_bytree': (0.6, 0.8),
        'learning_rate': (0.01, 0.1),
        'n_estimators': (100, 300),
        'num_leaves': (31, 60),
    }


    grid_search = GridSearchCV(estimator=fusion_model, param_grid=param_grid_fusion, scoring='accuracy', cv=5, n_jobs=-1)
    grid_search.fit(X_train_combined, y_train)
    best_params_grid_search = grid_search.best_params_


    bayes_search =BayesianOptimization(lgb_eval, param_space)
    bayes_search.maximize(init_points=5, n_iter=20)

    best_params_bayes = bayes_search.max['params']
    best_score_bayes = bayes_search.max['target']
    best_params_bayes['n_estimators'] = int(best_params_bayes['n_estimators'])
    best_params_bayes['max_depth'] = int(best_params_bayes['max_depth'])
    best_params_bayes['min_child_weight'] = int(best_params_bayes['min_child_weight'])
    best_params_bayes['num_leaves'] = int(best_params_bayes['num_leaves'])


    best_params_combined = {**best_params_grid_search, **best_params_bayes}
    print("Best parameters found for fusion model (finnal): ", best_params_combined)
    fusion_model.set_params(**best_params_combined)
    fusion_model.fit(X_train_combined, y_train)

    y_pred_lgbm_test = best_model.predict(X_test)
    y_pred_lr_test = lr.predict(X_test)
    y_pred_svc_test = svc.predict(X_test)
    y_pred_rf_test = rf.predict(X_test)

    X_test_combined = np.concatenate((X_test, y_pred_lgbm_test.reshape(-1, 1),
                                    y_pred_lr_test.reshape(-1, 1),
                                    y_pred_svc_test.reshape(-1, 1),
                                    y_pred_rf_test.reshape(-1, 1)), axis=1)

    best_model_fusion = fusion_model
    y_pred_fusion1 = best_model_fusion.predict_proba(X_test_combined)
    y_pred_fusion = best_model_fusion.predict(X_test_combined)

    dump(best_model_fusion, model_dir+'/best_model.joblib')
    dump(scaler, model_dir+'/scaler.joblib')
    dump(pca, model_dir+'/pca.joblib')
    dump(best_model, model_dir+'/lgbm_model.joblib')
    dump(lr, model_dir+'/lr_model.joblib')
    dump(svc, model_dir+'/svc_model.joblib')
    dump(rf, model_dir+'/rf_model.joblib')


if __name__ == "__main__":
    main()